{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"../models/bbox_cut/atpt_1_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/50309251_jpg.rf.be13d58da57890329c242f3bbce57346.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/50744549_jpg.rf.a02c89258c14024d6cffb86d5402a247.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/51174209_jpg.rf.63854ca575f09d0c8378b909c5833d41.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/51174951_jpg.rf.a23fc1c5c6507c54b82b3620ea871d2a.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/51176477_jpg.rf.853b10778efbf9a2948c3bf8e6ed7fcf.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/51225407_jpg.rf.d654f87ff5a83e626ee403b52d1d9032.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/51472579_jpg.rf.a7c7c9240c5a090a0ed015ae2c779be4.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/52010378_jpg.rf.61397ffc3539784d69ff85558ffcfdc0.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/52088507_jpg.rf.c20b84d03d68fc9b02a8bc67014fee2a.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/52132834_jpg.rf.0351493bf8e438f555a76f20abe17a4f.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/52276631_jpg.rf.b1ea96f2a98c67e83d65613d633b1e82.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/52720257_jpg.rf.9880b00470a3bdbf505dae71dec1940f.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/52959822_jpg.rf.7a90a013f9217e3c0462146432b03015.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/53143467_jpg.rf.deff559b7aae539c10de56643bd9d7c0.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/53183851_jpg.rf.7fa3f30199915859e5d415a31ace9642.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/53212484_jpg.rf.09fbc3e117336e775df7e3c27e9151ae.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/53386785_jpg.rf.cd924c211c789f4c190337dc739bce1c.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/53870622_jpg.rf.6b9a6dff164cf2ce08029bad2ee666d5.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/53876652_jpg.rf.3ef6f491a0e946caeb98956b0ea1db4c.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/54663471_jpg.rf.9e68e953264fd53600317848060c6323.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/54761655_jpg.rf.dd403fda4f03282ee0fd0366b4fab531.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/54906730_jpg.rf.41d335bc5a39fef5f53145677f25d843.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/55645659_jpg.rf.2bfac0c133cf66b8017a3e624be66ea9.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/55741110_jpg.rf.55eb560437afda976076336a3c742b6b.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/55803324_jpg.rf.a270799b80c66f749f734b7b75e72523.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/55987499_jpg.rf.b4153000cc42899f34107494bee56eaf.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/57022246_jpg.rf.acadb74378725f89bcb016ef19cb98f7.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/57444713_jpg.rf.cfbc58f7be9d914536b06e44be9381cb.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/57608895_jpg.rf.71dff38b0618d0e1225eb797e6667320.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/59604223_jpg.rf.ee4de8b7a6dc3d238f23504419b3ec79.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/60434933_jpg.rf.83317f6c3c949404ae541828a70f09da.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/60943180_jpg.rf.b12a68d69cdb876097df923c2195a29f.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/61120101_jpg.rf.e19156afa0e330cee610cda450dffe9d.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/61218657_jpg.rf.304404357e1ce3864ac78a6f3c779c61.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/61301321_jpg.rf.806b06dba77b31e485a08bcc83c364ac.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/61508016_jpg.rf.ce9d5e979cde1af9ef6e75491ba699b3.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/61644522_jpg.rf.dd7a2c0c8b1f3b45cc7a2b161042fb0e.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/61668737_jpg.rf.dcaf1e24f3d1a6a7e7aaa46eb5caed6e.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/62086566_jpg.rf.182ddc773c4b4d322182de34b204439e.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/62868070_jpg.rf.e18178ef35c262fcb6e5a814c1869e98.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/63003776_jpg.rf.bc302c1893f9892f13f1373aa7a57c5a.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/63518765_jpg.rf.dfc0fe0ace0fc2a10153f7a0a9eae0a5.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/64015142_jpg.rf.db7555f34cd97632969ccf2b14aafea4.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/65380750_jpg.rf.9833393c70a9372c1b45517d146dd964.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/65473381_jpg.rf.1a7ab25c8c67ddad2b43f71f7b63b7d0.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/68080947_jpg.rf.2559a44eba8a08e959fb12fa8fe00a75.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/68091370_jpg.rf.2f31cd119d4dcc345ddbf5042cd641b7.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/70711429_jpg.rf.84b3fdd09dfb5660d935704389112a74.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/70711551_jpg.rf.90ff60d68ac2ed1801377a71767066e1.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/76806835_jpg.rf.35abb20a46534582d1c3d75202ce3e78.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/76807288_jpg.rf.73215458a9be9d09001074ec6808fbee.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/94290822_jpg.rf.1a27d96954371ee588bbcae4d86d3087.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/95273926_jpg.rf.218fa461d229700b2f72fe4808ff3c2f.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/95911681_jpg.rf.fb8cb6de305e7e14801f6db04682d2eb.jpg', '../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images/96734934_jpg.rf.29926639d62a59a9e462b6812b3f9019.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Prepare test images\n",
    "test_dir = \"../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test/images\"\n",
    "images = [f'{test_dir}/{img}' for img in os.listdir(test_dir) if os.path.isfile(f'{test_dir}/{img}')]\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.49  Python-3.12.5 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4070 SUPER, 12282MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ARTEZON\\Desktop\\Универ\\Алгоритмы цифровой обработки мультимедиа\\ИЗ 3\\Train-Numbers-OCR\\datasets\\Train Numbers.v2-attempt-without-resize.yolov11\\test\\labels... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<00:00, 1018.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\ARTEZON\\Desktop\\\\   \\ 3\\Train-Numbers-OCR\\datasets\\Train Numbers.v2-attempt-without-resize.yolov11\\test\\labels.cache\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 52, len(boxes) = 55. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:07<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         55      0.912      0.564      0.695      0.441\n",
      "Speed: 1.6ms preprocess, 9.8ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0.44124])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model.val(data=os.path.abspath(\"../datasets/Train Numbers.v2-attempt-without-resize.yolov11/test.yaml\"), mode='test')\n",
    "metrics.box.map  # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps  # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 numbers, 4.8ms\n",
      "1: 640x640 1 numbers, 4.8ms\n",
      "2: 640x640 1 numbers, 4.8ms\n",
      "3: 640x640 1 numbers, 4.8ms\n",
      "4: 640x640 1 numbers, 4.8ms\n",
      "5: 640x640 1 numbers, 4.8ms\n",
      "6: 640x640 (no detections), 4.8ms\n",
      "7: 640x640 1 numbers, 4.8ms\n",
      "8: 640x640 1 numbers, 4.8ms\n",
      "9: 640x640 (no detections), 4.8ms\n",
      "10: 640x640 2 numberss, 4.8ms\n",
      "11: 640x640 2 numberss, 4.8ms\n",
      "12: 640x640 1 numbers, 4.8ms\n",
      "13: 640x640 1 numbers, 4.8ms\n",
      "14: 640x640 1 numbers, 4.8ms\n",
      "15: 640x640 1 numbers, 4.8ms\n",
      "16: 640x640 (no detections), 4.8ms\n",
      "17: 640x640 1 numbers, 4.8ms\n",
      "18: 640x640 1 numbers, 4.8ms\n",
      "19: 640x640 2 numberss, 4.8ms\n",
      "20: 640x640 1 numbers, 4.8ms\n",
      "21: 640x640 (no detections), 4.8ms\n",
      "22: 640x640 (no detections), 4.8ms\n",
      "23: 640x640 (no detections), 4.8ms\n",
      "24: 640x640 2 numberss, 4.8ms\n",
      "25: 640x640 1 numbers, 4.8ms\n",
      "26: 640x640 (no detections), 4.8ms\n",
      "27: 640x640 1 numbers, 4.8ms\n",
      "28: 640x640 3 numberss, 4.8ms\n",
      "29: 640x640 (no detections), 4.8ms\n",
      "30: 640x640 (no detections), 4.8ms\n",
      "31: 640x640 1 numbers, 4.8ms\n",
      "32: 640x640 1 numbers, 4.8ms\n",
      "33: 640x640 1 numbers, 4.8ms\n",
      "34: 640x640 2 numberss, 4.8ms\n",
      "35: 640x640 2 numberss, 4.8ms\n",
      "36: 640x640 1 numbers, 4.8ms\n",
      "37: 640x640 (no detections), 4.8ms\n",
      "38: 640x640 (no detections), 4.8ms\n",
      "39: 640x640 1 numbers, 4.8ms\n",
      "40: 640x640 1 numbers, 4.8ms\n",
      "41: 640x640 1 numbers, 4.8ms\n",
      "42: 640x640 (no detections), 4.8ms\n",
      "43: 640x640 1 numbers, 4.8ms\n",
      "44: 640x640 (no detections), 4.8ms\n",
      "45: 640x640 1 numbers, 4.8ms\n",
      "46: 640x640 1 numbers, 4.8ms\n",
      "47: 640x640 (no detections), 4.8ms\n",
      "48: 640x640 (no detections), 4.8ms\n",
      "49: 640x640 (no detections), 4.8ms\n",
      "50: 640x640 1 numbers, 4.8ms\n",
      "51: 640x640 2 numberss, 4.8ms\n",
      "52: 640x640 1 numbers, 4.8ms\n",
      "53: 640x640 2 numberss, 4.8ms\n",
      "54: 640x640 1 numbers, 4.8ms\n",
      "Speed: 3.5ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# Run batched inference on a list of images\n",
    "results = model(images)  # return a list of Results objects\n",
    "\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.save(filename=f\"atpt_1_results/{result.path.rsplit('/', 1)[1]}\")  # save to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
